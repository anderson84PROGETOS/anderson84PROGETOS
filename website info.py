import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from socket import socket, AF_INET, SOCK_STREAM
import re

print("""

â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— 
â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘ â–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—      â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â•      â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
â•šâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
 â•šâ•â•â•â•šâ•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•   â•šâ•â•   â•šâ•â•â•â•â•â•â•    â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• 
                                                                                        
""")

# Solicitar a URL do usuÃ¡rio
url = input("\nDigite a URL do website: ")

# User agent para simular requisiÃ§Ãµes feitas pelo navegador Firefox
user_agent_firefox = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'

# User agent para simular requisiÃ§Ãµes feitas pelo cURL
user_agent_curl = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'

# CabeÃ§alhos padrÃ£o para Firefox
headers_firefox = {
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'Referer': url,
    'User-Agent': user_agent_firefox
}

# CabeÃ§alhos padrÃ£o para cURL
headers_curl = {
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'Referer': url,
    'User-Agent': user_agent_curl
}

# FunÃ§Ã£o para fazer a requisiÃ§Ã£o e processar o conteÃºdo
def fetch_and_process(url, headers, user_agent_name):
    # Criar uma sessÃ£o para manter cookies e outras configuraÃ§Ãµes
    session = requests.Session()
    session.headers.update(headers)
    
    # Fazer a requisiÃ§Ã£o HTTP para obter o conteÃºdo da pÃ¡gina com os cabeÃ§alhos
    response = session.get(url)
    
    # Verificar o cÃ³digo de status e continuar mesmo que nÃ£o seja 200
    print(f"\n\nStatus da resposta HTTP ({user_agent_name}): {response.status_code}")

    # Extrair e imprimir os cabeÃ§alhos HTTP da resposta
    print(f"\n\n============= CabeÃ§alhos HTTP ({user_agent_name}) =============\n")
    for key, value in response.headers.items():
        print(f"{key}: {value}")
    
    # Se o user agent nÃ£o for o cURL, continuar com a extraÃ§Ã£o de conteÃºdo
    if user_agent_name != "CURL":
        # Analisar o conteÃºdo HTML da pÃ¡gina
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extrair e imprimir parÃ¡grafos
        print(f"\n\n============= ParÃ¡grafos ({user_agent_name}) =============\n")
        paragraphs = soup.find_all('p')
        for para in paragraphs:
            print(para.get_text())
        
        # Extrair e imprimir links
        print(f"\n\n============= Links ({user_agent_name}) =============\n")
        links = soup.find_all('a')
        for link in links:
            href = link.get('href')
            text = link.get_text().strip()
            if href:
                print(f"{text}: {href}")
        
        # Extrair e imprimir tÃ­tulos
        print(f"\n\n============= TÃ­tulos ({user_agent_name}) =============\n")
        titles = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        for title in titles:
            print(title.get_text().strip())
        
        # Extrair e imprimir imagens
        print(f"\n\n============= Imagens ({user_agent_name}) =============\n")
        images = soup.find_all('img')
        for img in images:
            src = img.get('src')
            alt = img.get('alt')
            if src:
                print(f"Fonte: {src}, Texto alternativo: {alt}")
        
        # Extrair e imprimir listas
        print(f"\n\n============= Listas ({user_agent_name}) =============\n")
        lists = soup.find_all(['ul', 'ol'])
        for lst in lists:
            items = lst.find_all('li')
            for item in items:
                print(item.get_text().strip())
        
        # Extrair e imprimir tabelas
        print(f"\n\n============= Tabelas ({user_agent_name}) =============\n")
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all(['td', 'th'])
                col_texts = [col.get_text().strip() for col in cols]
                print('\t'.join(col_texts))
        
        # Extrair e imprimir todos os outros textos
        print(f"\n\n============= Outros Textos ({user_agent_name}) =============\n")
        other_texts = soup.find_all(string=True)
        for text in other_texts:
            if text.parent.name not in ['script', 'style', 'head', 'title', 'meta', '[document]']:
                content = text.strip()
                if content:
                    print(content)
        
        # Obter e imprimir o conteÃºdo do robots.txt
        robots_url = urljoin(url, '/robots.txt')
        robots_response = session.get(robots_url)
        print(f"\n\n============= Robots.txt ({user_agent_name}) =============\n")
        if robots_response.status_code == 200:
            print(robots_response.text)
        else:
            print(f"NÃ£o foi possÃ­vel acessar o robots.txt ({user_agent_name}) (Status: {robots_response.status_code})")

# Usar o user agent do Firefox
print("\n\n\n\nğŸ¯=========================== Usando User Agent do Firefox ===========================ğŸ¯\n")
fetch_and_process(url, headers_firefox, "Firefox")

# Usar o user agent do cURL
print("\n\n\n\nğŸ¯=========================== Usando User Agent do CURL ===========================ğŸ¯\n")
fetch_and_process(url, headers_curl, "CURL")

# FunÃ§Ã£o para requisitar informaÃ§Ãµes WHOIS
def requisicao_whois(servidor_whois, endereco_host, padrao):
    objeto_socket = socket(AF_INET, SOCK_STREAM)
    conexao = objeto_socket.connect_ex((servidor_whois, 43))
    if conexao == 0:
        if padrao:
            if servidor_whois == 'whois.verisign-grs.com':  # For .com and .net domains
                objeto_socket.send(f'domain {endereco_host}\r\n'.encode())
            else:
                objeto_socket.send(f'n + {endereco_host}\r\n'.encode())
        else:
            objeto_socket.send(f'{endereco_host}\r\n'.encode())
        
        while True:
            dados = objeto_socket.recv(65500)
            if not dados:
                break
            print(dados.decode('latin-1'))
    objeto_socket.close()

# FunÃ§Ã£o para obter WHOIS para domÃ­nios .gov
def obter_whois_gov(endereco):
    servidores_whois_tdl = {
        '.gov': 'whois.nic.gov'
    }
    servidor_whois_gov = servidores_whois_tdl.get('.gov', None)
    if servidor_whois_gov:
        requisicao_whois(servidor_whois_gov, endereco, False)
    else:
        print("Servidor WHOIS para domÃ­nios .gov nÃ£o encontrado.")

# FunÃ§Ã£o para encontrar e-mails em um BeautifulSoup object
def encontrar_emails(soup):
    email_regex = r"[\w\.-]+@[\w\.-]+"
    emails = []

    # Procura e retorna os e-mails na pÃ¡gina principal do WHOIS
    email_section = soup.find("div", class_="row-fluid registry-data")
    if email_section:
        email_text = email_section.find_all("div", class_="row")[1].find("div", class_="span9").get_text()
        email_matches = re.findall(email_regex, email_text)
        emails.extend(email_matches)

    # Procura e retorna os e-mails no resultado completo do WHOIS
    whois_section = soup.find("pre", class_="df-raw")
    if whois_section:
        whois_text = whois_section.get_text()
        email_matches = re.findall(email_regex, whois_text)
        emails.extend(email_matches)

    return emails

# FunÃ§Ã£o para extrair campos especÃ­ficos do WHOIS
def extrair_campo(whois_section, label):
    field = whois_section.find("div", string=re.compile(label))
    if field:
        value = field.find_next_sibling("div").get_text(strip=True)
        return value
    return ""

# FunÃ§Ã£o para obter informaÃ§Ãµes WHOIS
def obter_whois(endereco):
    url_whois = f"https://www.whois.com/whois/{endereco}"
    url_registro_br = f"https://registro.br/cgi-bin/whois/?qr={endereco}"

    response_whois = requests.get(url_whois)
    response_registro_br = requests.get(url_registro_br)

    if response_whois.status_code == 200:
        soup_whois = BeautifulSoup(response_whois.text, "html.parser")
        whois_section = soup_whois.find("pre", class_="df-raw")
        if whois_section:
            whois_text = whois_section.get_text()
            print(whois_text)

            # Extract and display additional information
            emails = encontrar_emails(soup_whois)
            if emails:
                print("\nE-mails encontrados:")
                for email in emails:
                    print(email)

            # Extract more fields if needed
            name = extrair_campo(whois_section, "Registrant Name:")
            registration_date = extrair_campo(whois_section, "Creation Date:")
            expiration_date = extrair_campo(whois_section, "Registrar Registration Expiration Date:")

            if name:
                print("Nome do Titular:", name)
            if registration_date:
                print("Data de Registro:", registration_date)
            if expiration_date:
                print("Data de ExpiraÃ§Ã£o:", expiration_date)

    if response_registro_br.status_code == 200:
        soup_registro_br = BeautifulSoup(response_registro_br.text, "html.parser")
        div_result = soup_registro_br.find("div", class_="result")
        if div_result:
            result_text = div_result.get_text()
            print(result_text)

def obter_whois_br(endereco):
    servidores_whois_tdl = {
        '.br': 'whois.registro.br'
    }
    servidor_whois = servidores_whois_tdl['.br']
    requisicao_whois(servidor_whois, endereco, False)

# FunÃ§Ã£o principal para executar as operaÃ§Ãµes
def main():
    endereco = url.replace('http://', '').replace('https://', '').split('/')[0]
    
    print("\n\n\n\n========================================== Consulta Whois ==========================================\n")
    obter_whois_br(endereco)
    obter_whois(endereco)
    if re.search(r'\.gov$', endereco):
        obter_whois_gov(endereco)

# Executar a funÃ§Ã£o principal
if __name__ == "__main__":
    main()

input("\n\nğŸ¯ Pressione Enter para sair ğŸ¯\n")
